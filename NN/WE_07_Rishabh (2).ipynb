{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ee849c",
   "metadata": {},
   "source": [
    "## 1.0 SETUP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e75ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and pandas libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36832d",
   "metadata": {},
   "source": [
    "## 2.0 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d24ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./train_X.csv\")\n",
    "X_test = pd.read_csv(\"./test_X.csv\")\n",
    "y_train = pd.read_csv(\"./train_y.csv\")\n",
    "y_test = pd.read_csv(\"./test_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16aeebf",
   "metadata": {},
   "source": [
    "### For predicting whether an NBA player will play for 5 years or not, the more relevant metric to use would be Precision.\n",
    "\n",
    "**Precision** is the proportion of true positives out of all the predicted positives. \n",
    "In our case, a forecast that a player will play for at least five years would be considered a true positive, whereas a prediction that a player will play for at least five years but eventually retire or sustain a career-ending injury would be considered a false positive.\n",
    "\n",
    "The reason I chose precision and why precision is more relevant in this case is that it is more important to avoid false positives (i.e., predicting a player will play for 5 years when they won't) than to avoid false negatives (i.e., predicting a player won't play for 5 years when they will).\n",
    "\n",
    "I went with **precision** since it's more essential to prevent false positives (for example, forecasting a player will play for 5 years when they won't) than false negatives (for example, predicting a player won't play for 5 years when they will) in this situation.\n",
    "\n",
    "This is because false positives can lead to significant financial losses for NBA teams if they invest in players who retire early or suffer career-ending injuries, while investing in players who are predicted to play for less than 5 years, but end up playing for longer is not a financial loss for the teams.\n",
    "\n",
    "Therefore, precision is the more relevant metric to use when evaluating the performance of a predictive model for whether NBA players will play for 5 years or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93f95d",
   "metadata": {},
   "source": [
    "## 3.0 Model the data\n",
    "First, we will create a dataframe to hold all the results of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4cdf6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d31ae5",
   "metadata": {},
   "source": [
    "## 3.1.1 Logistic regression using random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72350f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 0.7033599200498099\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 601}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "1255 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "645 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "340 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "270 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.69690454        nan        nan        nan\n",
      " 0.69690454 0.70335992 0.70335992        nan        nan        nan\n",
      " 0.69690454        nan 0.69690454 0.69690454 0.70335992        nan\n",
      "        nan        nan        nan 0.69690454 0.70335992 0.70335992\n",
      " 0.69690454        nan        nan        nan 0.70335992        nan\n",
      " 0.70335992 0.70335992 0.70335992        nan        nan        nan\n",
      "        nan        nan 0.69690454 0.70335992 0.70335992        nan\n",
      " 0.70335992        nan        nan        nan        nan        nan\n",
      " 0.69690454 0.69690454        nan        nan        nan        nan\n",
      " 0.69690454 0.69690454 0.70335992 0.70335992        nan 0.70335992\n",
      " 0.70335992        nan 0.69690454        nan        nan 0.69690454\n",
      " 0.70335992 0.69690454 0.70335992 0.69690454        nan        nan\n",
      " 0.70335992 0.70335992 0.69690454 0.69690454        nan        nan\n",
      " 0.70335992 0.70335992 0.70335992 0.69690454        nan        nan\n",
      " 0.69690454 0.69690454        nan 0.69690454 0.69690454 0.69690454\n",
      " 0.69690454 0.69690454 0.69690454 0.70335992 0.69690454 0.69690454\n",
      " 0.70335992        nan 0.69690454 0.69690454        nan        nan\n",
      "        nan 0.70335992 0.70335992 0.70335992        nan 0.70335992\n",
      " 0.70335992 0.69690454        nan        nan 0.69690454 0.69690454\n",
      " 0.69690454        nan 0.69690454 0.70335992        nan        nan\n",
      "        nan 0.70335992 0.70335992        nan        nan 0.70335992\n",
      " 0.69690454 0.69690454        nan 0.70335992 0.69690454        nan\n",
      "        nan 0.70335992 0.69690454        nan        nan 0.69690454\n",
      " 0.70335992        nan        nan        nan 0.69690454        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69690454        nan 0.70335992        nan 0.70335992        nan\n",
      "        nan 0.69690454        nan        nan 0.70335992        nan\n",
      " 0.69690454 0.69690454        nan        nan        nan 0.70335992\n",
      "        nan        nan 0.70335992        nan        nan 0.69690454\n",
      " 0.70335992 0.69690454        nan 0.70335992 0.69690454 0.70335992\n",
      "        nan 0.69690454        nan 0.69690454        nan        nan\n",
      "        nan 0.70335992        nan        nan 0.70335992 0.69690454\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.69690454 0.70335992        nan 0.69690454\n",
      "        nan 0.69690454 0.70335992        nan 0.70335992 0.69690454\n",
      " 0.69690454 0.70335992 0.70335992        nan 0.69690454        nan\n",
      "        nan        nan 0.70335992 0.69690454 0.70335992 0.70335992\n",
      " 0.70335992        nan        nan 0.70335992        nan        nan\n",
      "        nan 0.69690454        nan        nan        nan 0.70335992\n",
      "        nan 0.70335992 0.69690454        nan 0.70335992 0.70335992\n",
      "        nan        nan        nan        nan 0.70335992 0.70335992\n",
      " 0.69690454 0.70335992        nan 0.69690454 0.69690454 0.69690454\n",
      "        nan 0.70335992 0.70335992 0.69690454        nan 0.70335992\n",
      " 0.70335992 0.69690454 0.70335992        nan        nan        nan\n",
      "        nan 0.69690454        nan        nan        nan        nan\n",
      " 0.70335992        nan        nan        nan        nan 0.70335992\n",
      "        nan 0.70335992        nan        nan 0.69690454        nan\n",
      " 0.70335992        nan 0.69690454 0.70335992        nan        nan\n",
      " 0.70335992 0.70335992 0.70335992        nan 0.69690454 0.69690454\n",
      " 0.70335992        nan 0.69690454 0.69690454        nan 0.70335992\n",
      "        nan        nan 0.69690454 0.69690454        nan        nan\n",
      " 0.69690454 0.70335992        nan 0.69690454        nan        nan\n",
      "        nan        nan 0.70335992 0.69690454        nan        nan\n",
      "        nan 0.69690454        nan        nan 0.69690454 0.69690454\n",
      "        nan        nan 0.69690454        nan 0.69690454        nan\n",
      " 0.69690454        nan        nan 0.69690454 0.69690454 0.70335992\n",
      "        nan        nan        nan 0.70335992 0.69690454        nan\n",
      "        nan 0.70335992 0.69690454        nan 0.69690454        nan\n",
      "        nan        nan        nan 0.70335992        nan 0.69690454\n",
      "        nan 0.70335992        nan        nan        nan        nan\n",
      " 0.70335992        nan        nan 0.69690454        nan 0.70335992\n",
      " 0.70335992        nan        nan        nan 0.69690454 0.69690454\n",
      " 0.69690454 0.70335992        nan        nan        nan        nan\n",
      " 0.69690454 0.70335992        nan        nan 0.69690454 0.69690454\n",
      "        nan 0.69690454        nan        nan 0.70335992 0.69690454\n",
      "        nan        nan        nan 0.70335992        nan 0.69690454\n",
      " 0.70335992        nan 0.70335992 0.69690454        nan 0.70335992\n",
      "        nan 0.69690454        nan 0.70335992        nan 0.69690454\n",
      "        nan 0.70335992 0.69690454 0.69690454        nan 0.69690454\n",
      " 0.69690454        nan        nan        nan 0.69690454 0.70335992\n",
      "        nan 0.69690454 0.69690454 0.70335992 0.69690454 0.70335992\n",
      " 0.70335992        nan 0.69690454 0.69690454 0.69690454 0.70335992\n",
      "        nan        nan        nan        nan 0.69690454        nan\n",
      "        nan 0.69690454 0.70335992 0.69690454        nan 0.70335992\n",
      "        nan 0.70335992        nan        nan        nan 0.69690454\n",
      " 0.70335992 0.70335992        nan 0.70335992        nan 0.69690454\n",
      "        nan        nan        nan 0.69690454 0.70335992 0.70335992\n",
      "        nan        nan        nan 0.69690454        nan 0.69690454\n",
      "        nan        nan 0.70335992 0.69690454 0.70335992 0.69690454\n",
      " 0.69690454        nan        nan        nan        nan 0.69690454\n",
      " 0.69690454 0.69690454 0.69690454        nan        nan        nan\n",
      "        nan        nan 0.70335992        nan        nan        nan\n",
      " 0.69690454        nan        nan 0.69690454 0.69690454        nan\n",
      " 0.69690454 0.70335992]\n",
      "  warnings.warn(\n",
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan        nan 0.7133131         nan        nan        nan\n",
      " 0.7133131  0.71998661 0.71998661        nan        nan        nan\n",
      " 0.7133131         nan 0.71573579 0.7133131  0.71998661        nan\n",
      "        nan        nan        nan 0.7133131  0.71998661 0.71998661\n",
      " 0.71573579        nan        nan        nan 0.71998661        nan\n",
      " 0.71998661 0.71998661 0.71998661        nan        nan        nan\n",
      "        nan        nan 0.7133131  0.71998661 0.71998661        nan\n",
      " 0.71998661        nan        nan        nan        nan        nan\n",
      " 0.7133131  0.71573579        nan        nan        nan        nan\n",
      " 0.71573579 0.71573579 0.71998661 0.71998661        nan 0.71998661\n",
      " 0.71998661        nan 0.71573579        nan        nan 0.71573579\n",
      " 0.71998661 0.7133131  0.71998661 0.71573579        nan        nan\n",
      " 0.71998661 0.71998661 0.7133131  0.71573579        nan        nan\n",
      " 0.71998661 0.71998661 0.71998661 0.71573579        nan        nan\n",
      " 0.7133131  0.7133131         nan 0.7133131  0.71573579 0.7133131\n",
      " 0.71573579 0.71573579 0.7133131  0.71998661 0.71573579 0.7133131\n",
      " 0.71998661        nan 0.7133131  0.7133131         nan        nan\n",
      "        nan 0.71998661 0.71998661 0.71998661        nan 0.71998661\n",
      " 0.71998661 0.71573579        nan        nan 0.71573579 0.71573579\n",
      " 0.71573579        nan 0.71573579 0.71998661        nan        nan\n",
      "        nan 0.71998661 0.71998661        nan        nan 0.71998661\n",
      " 0.71573579 0.7133131         nan 0.71998661 0.71573579        nan\n",
      "        nan 0.71998661 0.71573579        nan        nan 0.7133131\n",
      " 0.71998661        nan        nan        nan 0.7133131         nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71573579        nan 0.71998661        nan 0.71998661        nan\n",
      "        nan 0.71573579        nan        nan 0.71998661        nan\n",
      " 0.7133131  0.7133131         nan        nan        nan 0.71998661\n",
      "        nan        nan 0.71998661        nan        nan 0.7133131\n",
      " 0.71998661 0.7133131         nan 0.71998661 0.71573579 0.71998661\n",
      "        nan 0.71573579        nan 0.71573579        nan        nan\n",
      "        nan 0.71998661        nan        nan 0.71998661 0.71573579\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.71573579 0.71998661        nan 0.71573579\n",
      "        nan 0.71573579 0.71998661        nan 0.71998661 0.71573579\n",
      " 0.7133131  0.71998661 0.71998661        nan 0.7133131         nan\n",
      "        nan        nan 0.71998661 0.7133131  0.71998661 0.71998661\n",
      " 0.71998661        nan        nan 0.71998661        nan        nan\n",
      "        nan 0.71573579        nan        nan        nan 0.71998661\n",
      "        nan 0.71998661 0.71573579        nan 0.71998661 0.71998661\n",
      "        nan        nan        nan        nan 0.71998661 0.71998661\n",
      " 0.71573579 0.71998661        nan 0.71573579 0.71573579 0.71573579\n",
      "        nan 0.71998661 0.71998661 0.71573579        nan 0.71998661\n",
      " 0.71998661 0.7133131  0.71998661        nan        nan        nan\n",
      "        nan 0.71573579        nan        nan        nan        nan\n",
      " 0.71998661        nan        nan        nan        nan 0.71998661\n",
      "        nan 0.71998661        nan        nan 0.71573579        nan\n",
      " 0.71998661        nan 0.71573579 0.71998661        nan        nan\n",
      " 0.71998661 0.71998661 0.71998661        nan 0.71573579 0.7133131\n",
      " 0.71998661        nan 0.7133131  0.71573579        nan 0.71998661\n",
      "        nan        nan 0.71573579 0.71573579        nan        nan\n",
      " 0.71573579 0.71998661        nan 0.7133131         nan        nan\n",
      "        nan        nan 0.71998661 0.71573579        nan        nan\n",
      "        nan 0.7133131         nan        nan 0.71573579 0.71573579\n",
      "        nan        nan 0.71573579        nan 0.7133131         nan\n",
      " 0.7133131         nan        nan 0.7133131  0.7133131  0.71998661\n",
      "        nan        nan        nan 0.71998661 0.71573579        nan\n",
      "        nan 0.71998661 0.7133131         nan 0.71573579        nan\n",
      "        nan        nan        nan 0.71998661        nan 0.7133131\n",
      "        nan 0.71998661        nan        nan        nan        nan\n",
      " 0.71998661        nan        nan 0.7133131         nan 0.71998661\n",
      " 0.71998661        nan        nan        nan 0.7133131  0.7133131\n",
      " 0.7133131  0.71998661        nan        nan        nan        nan\n",
      " 0.7133131  0.71998661        nan        nan 0.71573579 0.71573579\n",
      "        nan 0.71573579        nan        nan 0.71998661 0.7133131\n",
      "        nan        nan        nan 0.71998661        nan 0.71573579\n",
      " 0.71998661        nan 0.71998661 0.7133131         nan 0.71998661\n",
      "        nan 0.71573579        nan 0.71998661        nan 0.71573579\n",
      "        nan 0.71998661 0.7133131  0.7133131         nan 0.71573579\n",
      " 0.7133131         nan        nan        nan 0.71573579 0.71998661\n",
      "        nan 0.7133131  0.71573579 0.71998661 0.7133131  0.71998661\n",
      " 0.71998661        nan 0.71573579 0.7133131  0.71573579 0.71998661\n",
      "        nan        nan        nan        nan 0.7133131         nan\n",
      "        nan 0.71573579 0.71998661 0.71573579        nan 0.71998661\n",
      "        nan 0.71998661        nan        nan        nan 0.71573579\n",
      " 0.71998661 0.71998661        nan 0.71998661        nan 0.71573579\n",
      "        nan        nan        nan 0.7133131  0.71998661 0.71998661\n",
      "        nan        nan        nan 0.7133131         nan 0.71573579\n",
      "        nan        nan 0.71998661 0.7133131  0.71998661 0.7133131\n",
      " 0.71573579        nan        nan        nan        nan 0.71573579\n",
      " 0.7133131  0.7133131  0.7133131         nan        nan        nan\n",
      "        nan        nan 0.71998661        nan        nan        nan\n",
      " 0.71573579        nan        nan 0.7133131  0.7133131         nan\n",
      " 0.7133131  0.71998661]\n",
      "  warnings.warn(\n",
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'max_iter':np.arange(500,1000),\n",
    "    'penalty': ['None','l1','l2','elasticnet'],\n",
    "    'solver':['saga','liblinear']\n",
    "}\n",
    "\n",
    "log_reg_model = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = log_reg_model, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93527f0",
   "metadata": {},
   "source": [
    "## 3.1.2 Logistic regression using grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a84387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "The best precision score is 0.7033599200498099\n",
      "... with parameters: {'max_iter': 596, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "max_iter = rand_search.best_params_['max_iter']\n",
    "penalty = rand_search.best_params_['penalty']\n",
    "solver = rand_search.best_params_['solver']\n",
    "\n",
    "param_grid = {\n",
    "    'max_iter': np.arange(max_iter-5,max_iter+5),  \n",
    "    'penalty': [penalty],\n",
    "    'solver': [solver]\n",
    "}\n",
    "\n",
    "logistic_reg_model = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = logistic_reg_model, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallLogistic = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02da1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic Regression\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "log_reg_bm=grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e1989",
   "metadata": {},
   "source": [
    "## 3.2.1 Decision Tree using random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa63225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n",
      "The best precision score is 0.7491206162265629\n",
      "... with parameters: {'min_samples_split': 4, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0001, 'max_leaf_nodes': 58, 'max_depth': 16, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 350.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.67788661 0.6890523  0.67132506 0.70642164 0.70315638 0.70444991\n",
      " 0.69873944 0.69760937 0.6986813  0.67853129 0.68411707 0.70618003\n",
      " 0.74912062 0.6890523  0.70014554 0.67668404 0.69873944 0.69334106\n",
      " 0.69939779 0.70236348 0.68945898 0.67853129        nan 0.69108872\n",
      " 0.70113773 0.71763915 0.71299015 0.68411707 0.69517483 0.68796759\n",
      " 0.69675652 0.69184352 0.69103353 0.69873944 0.70404969 0.67206007\n",
      " 0.70113773 0.70573199 0.71974937 0.69939779 0.67242924 0.69973193\n",
      " 0.71974937 0.66771833 0.67668404 0.6870828  0.69154636 0.68106644\n",
      " 0.71389988 0.69552985 0.70404969 0.70954763 0.69966817 0.69904766\n",
      " 0.67878189 0.69450314 0.71406818 0.70404969 0.70113773 0.67838787\n",
      " 0.69809712 0.70236348 0.69873944 0.71307643 0.67853129 0.68659587\n",
      " 0.70236624 0.70033208 0.69856668 0.70053053]\n",
      "  warnings.warn(\n",
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.72709878 0.72041431 0.71003593 0.78515124 0.74562536 0.77869546\n",
      " 0.71191914 0.71921824 0.74131058 0.68675689 0.71782609 0.7443632\n",
      " 0.86963632 0.72041431 0.74596905 0.72538562 0.71191914 0.7275271\n",
      " 0.72998212 0.72720194 0.73878413 0.68675689        nan 0.70767497\n",
      " 0.72980627 0.7361229  0.74823078 0.71782609 0.72132719 0.71677468\n",
      " 0.79035478 0.74013954 0.73087374 0.71191914 0.72640111 0.71925649\n",
      " 0.72980627 0.80676946 0.74440097 0.72998212 0.7235371  0.72706827\n",
      " 0.74440097 0.70365512 0.72538562 0.74403156 0.75078911 0.7219833\n",
      " 0.77153848 0.76388846 0.72640111 0.74777155 0.73216278 0.78285969\n",
      " 0.72726044 0.75036675 0.73475471 0.72640111 0.72980627 0.72728205\n",
      " 0.75337306 0.72720194 0.71191914 0.7341439  0.68675689 0.73191862\n",
      " 0.73223161 0.71229231 0.71993164 0.74041331]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,20), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=70,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872cf8d5",
   "metadata": {},
   "source": [
    "## 3.2.2 Decision Tree using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b71d811a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1024 candidates, totalling 5120 fits\n",
      "The best precision score is 0.757398733357769\n",
      "... with parameters: {'criterion': 'gini', 'max_depth': 14, 'max_leaf_nodes': 57, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 3, 'min_samples_split': 4}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "min_samples_split = rand_search.best_params_['min_samples_split']\n",
    "min_samples_leaf = rand_search.best_params_['min_samples_leaf']\n",
    "min_impurity_decrease = rand_search.best_params_['min_impurity_decrease']\n",
    "max_leaf_nodes = rand_search.best_params_['max_leaf_nodes']\n",
    "max_depth = rand_search.best_params_['max_depth']\n",
    "criterion = rand_search.best_params_['criterion']\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(min_samples_split-2,min_samples_split+2),  \n",
    "    'min_samples_leaf': np.arange(min_samples_leaf-2,min_samples_leaf+2),\n",
    "    'min_impurity_decrease': np.arange(min_impurity_decrease-0.0001, min_impurity_decrease+0.0001, 0.00005),\n",
    "    'max_leaf_nodes': np.arange(max_leaf_nodes-2,max_leaf_nodes+2), \n",
    "    'max_depth': np.arange(max_depth-2,max_depth+2), \n",
    "    'criterion': [criterion]\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6776b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Decision Tree\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63910822",
   "metadata": {},
   "source": [
    "## 3.3.1 SVM using random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d51c9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 144 is smaller than n_iter=500. Running 144 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best precision score is 0.7779774253075223\n",
      "... with parameters: {'kernel': 'rbf', 'gamma': 'auto', 'C': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(1,25),   \n",
    "    'gamma': ['scale','auto'],\n",
    "    'kernel':['linear','rbf','poly']\n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = svm_model, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017815a",
   "metadata": {},
   "source": [
    "## 3.3.2 SVM using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1ef34b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "The best precision score is 0.7779774253075223\n",
      "... with parameters: {'C': 15, 'gamma': 'auto', 'kernel': 'rbf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "C = rand_search.best_params_['C']\n",
    "gamma = rand_search.best_params_['gamma']\n",
    "kernel = rand_search.best_params_['kernel']\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(C-2,C+2),  \n",
    "    'gamma': [gamma],\n",
    "    'kernel': [kernel]\n",
    "    \n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_model, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallSVM = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e29b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"SVM\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c4f8e",
   "metadata": {},
   "source": [
    "## Neural network using Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bef8c5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'sgd', 'max_iter': 5000, 'learning_rate_init': 0.2, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (40, 20), 'alpha': 0, 'activation': 'tanh'}\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,), (70,), (40,20)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0, .2, .5, .7, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.2],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = RandomizedSearchCV(estimator = ann, param_distributions=param_grid, cv=kfolds, n_iter=50,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e97009",
   "metadata": {},
   "source": [
    "## Neural network using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70e97173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling', 'learning_rate_init': 0.005, 'max_iter': 5000, 'solver': 'adam'}\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (30,), (50,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [.5, .7, 1],\n",
    "    'learning_rate': ['adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.005, 0.01, 0.15],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = GridSearchCV(estimator = ann, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec20d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Neural Network\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e45ec",
   "metadata": {},
   "source": [
    "## 3.4 Evaluating the performance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3069c5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.669154</td>\n",
       "      <td>0.778325</td>\n",
       "      <td>0.642276</td>\n",
       "      <td>0.703786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.651741</td>\n",
       "      <td>0.726496</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>0.718310</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.642276</td>\n",
       "      <td>0.702222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  Accuracy  Precision    Recall        F1\n",
       "0  Logistic Regression  0.669154   0.778325  0.642276  0.703786\n",
       "0        Decision Tree  0.651741   0.726496  0.691057  0.708333\n",
       "0                  SVM  0.619403   0.718310  0.621951  0.666667\n",
       "0       Neural Network  0.666667   0.774510  0.642276  0.702222"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e0fee",
   "metadata": {},
   "source": [
    "## 4.0 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e3cba",
   "metadata": {},
   "source": [
    "I applied **Neural network** model alongwith Decision Tree, SVM and logitic regression. As we can see from the results, Logistic Regression still performed best with a precision of *0.778*, when precision is used as the performance metric.\n",
    "\n",
    "However, Neural network model also performed really well in comparison to the other models with the second best precision value of *0.774* , accuracy of *0.666* ,and a good Recall and F1 score of *0.642* and *0.702* resp."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
